{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3CRKhawJgDi",
        "outputId": "10221123-c8ba-4180-c351-fb395622f554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bpOT67HKTa_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('/content/drive/My Drive/abcd/Reviews.csv')\n",
        "col=['Summary','Text']\n",
        "data[col].to_csv('Modified_review.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-oJtWSoLBrw"
      },
      "outputs": [],
      "source": [
        "del data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUJv5PmQLGl_",
        "outputId": "9a8f434b-8781-4cee-f401-e12ffb174a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/abcd/Modified_review.csv')\n",
        "rows=df[df['Summary'].isna() | df['Text'].isna()].shape[0]\n",
        "print(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmZoRMysLQgA"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset=['Summary','Text'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPt1VN03LUP4",
        "outputId": "5a85ec42-8d4e-4a62-daf4-1ced44357da2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uaMG1YvLbo5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svO7jVfdLg14"
      },
      "outputs": [],
      "source": [
        "def html_remove(text):\n",
        "    word=BeautifulSoup(text,\"html.parser\")\n",
        "    return word.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thSajynkLl8o"
      },
      "outputs": [],
      "source": [
        "def accented(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03o8o_5qLmFA",
        "outputId": "b05b54d7-1553-488d-b30d-6d2edd42bbbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-262890ab649b>:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  word=BeautifulSoup(text,\"html.parser\")\n",
            "<ipython-input-8-262890ab649b>:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  word=BeautifulSoup(text,\"html.parser\")\n",
            "<ipython-input-8-262890ab649b>:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  word=BeautifulSoup(text,\"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "def preprocess(text):\n",
        "    text = html_remove(text)\n",
        "    text = accented(text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "df['Summary'] = df['Summary'].apply(preprocess)\n",
        "df['Text'] = df['Text'].apply(preprocess)\n",
        "\n",
        "df.to_csv('Preprocessed_Review1.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2ubJ7nKLtjI"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Preprocessed_Review1.csv')\n",
        "sampled_df = df.sample(frac=0.30, random_state=42)\n",
        "\n",
        "sampled_df.to_csv('/content/drive/My Drive/abcd/less_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbfbap1MLtng"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(sampled_df, test_size=0.25, random_state=42)\n",
        "\n",
        "train_df.to_csv('/content/drive/My Drive/abcd/train.csv', index=False)\n",
        "test_df.to_csv('/content/drive/My Drive/abcd/test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NfeHWehPsZD",
        "outputId": "c94c45e6-b675-4ff1-a064-6e0e2aef2d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127896\n",
            "42632\n"
          ]
        }
      ],
      "source": [
        "n1=train_df.shape[0]\n",
        "n2=test_df.shape[0]\n",
        "print(n1)\n",
        "print(n2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOs_x0C5o__R",
        "outputId": "8b36fd99-41aa-4536-c218-7d3f081e85b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9ZHF1wIK6Aj",
        "outputId": "9974dc5d-339f-4212-d6a6-ddb6125599ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.29.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd9WDW8Su0Wj",
        "outputId": "0e4bf5ee-c83a-40b3-ef53-14b8d8bd2508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Y-_uWhMUhS",
        "outputId": "00bd1165-d150-4473-e5aa-9150f53bd635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: accelerate\n",
            "Version: 0.29.3\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: zach.mueller@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwfiTpHULtrR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux3fO310WJN-"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erc5ialCexyG"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/My Drive/abcd/train_output'\n",
        "model = load_model(model_path)\n",
        "tokenizer = load_tokenizer(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF-MVQs80iPO",
        "outputId": "6a437e45-9d47-4e73-9ea3-1837508a0586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: highly recommend rescue remedy purchased product friend dog 3yr old 85 lb male yellow lab named jack jack always frightened thunderstorm would pant drool shed fur tremble friend applied 4 drop treat gave jack within moment mood settled calm peaceful theyve two thunderstorm since giving jack rescue remedy time calm seems unbothered weather jack bit groggy next day side affect noticed\n",
            "Summary: work like charm\n",
            "\n",
            "Text: greenies 96 received quickly protective box fresh resealable bag would order vendor\n",
            "Summary: greenies dental chew\n",
            "\n",
            "Text: use litter automatic litter box work wonder clump hard clear away clean leaf odor behind bit dusty pour bunch dont find worse others ive tried occasion tried another brand litter always severely disappointed highly recommend litter especially automatic litter box\n",
            "Summary: strong clumping great smell\n",
            "\n",
            "Text: exactly ordered super fast delivery much easier grocery shopping expecially common ingredient\n",
            "Summary: great\n",
            "\n",
            "Text: perfect new puppy love smell really attracts chew puppy earlier commentator said polish good buy\n",
            "Summary: tasty treat new pup\n",
            "\n",
            "Text: dog loved food rash went away unfortunately rich found gained weight switch food weight management suggest food dog allergy active dog dog quite active enough burn food\n",
            "Summary: dog loved\n",
            "\n",
            "Text: pot roast cooked perfection flavor fabulous ive bought time every one fantastic first always use oven directed heat always come soooo tender tasty omaha steak great reputation food item greatexcept onei love despite relatively high cost sale right 20 amazon give tryyoull happy like pot roast course\n",
            "Summary: really delicious tender\n",
            "\n",
            "Text: unless real live pop rock actually explode mouth im interested fire stix\n",
            "Summary: yeah got pop rock\n",
            "\n",
            "Text: used product several year found single best supplement use losing maintaining weight correct metabolism level keep appetite control proprietary blend natural herbal booster nutrient used healthy diet plan dual action effect weight loss lastly priced well\n",
            "Summary: green tea fat metabolizer\n",
            "\n",
            "Text: soup pretty goodthere nothing wrong iti wish could slightly thicker use one two can soup big bowl thicker watered downadd salt pepper tabasco extremely tasty meal take coupe minute heat dont heat metalplastic container pour everything microwave save bowl heat half way stir continue total time 34 minute 1000 watt microwave im kidding add salt pepper tabasco like restaurant cant tell difference stuff stuff make fisherman wharf joke45 star thicker deal\n",
            "Summary: pretty good\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "texts=[]\n",
        "summaries=[]\n",
        "df1=pd.read_csv(\"/content/drive/My Drive/abcd/train.csv\")\n",
        "\n",
        "num_rows = len(df1)\n",
        "random_indices = random.sample(range(num_rows), 10)\n",
        "index_counter = 0\n",
        "while len(texts) < 10:\n",
        "    random_index = random_indices[index_counter]\n",
        "    text = df1.iloc[random_index]['Text']\n",
        "    summary = df1.iloc[random_index]['Summary']\n",
        "    texts.append(text)\n",
        "    summaries.append(summary)\n",
        "    index_counter += 1\n",
        "\n",
        "for text, summary in zip(texts, summaries):\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Summary:\", summary)\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "T5y2lpuN7k0Q",
        "outputId": "a83692b4-7592-4072-d320-7c8450913425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "Successfully installed tokenizers-0.19.1 transformers-4.40.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "397e678228104d77a7d3c949cb9cfb6d",
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz_8rwjxiTj6",
        "outputId": "a948c986-a463-417e-d832-03ee83808bdc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2LMHeadModel, AdamW\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class Modified_Set(Dataset):\n",
        "    def __init__(self, texts, summaries, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        summary = self.summaries[idx]\n",
        "        combined_text = (text, '[SEP]', summary)\n",
        "        encoding = self.tokenizer(combined_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        input_ids = encoding.input_ids.squeeze()\n",
        "        attention_mask = encoding.attention_mask.squeeze()\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "\n",
        "modified_data= Modified_Set(texts, summaries, tokenizer)\n",
        "\n",
        "batch_size = 2\n",
        "dataloader = DataLoader(modified_data, batch_size=batch_size, shuffle=True)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 3\n",
        "model.train()\n",
        "\n",
        "def forward_pass(model, input_ids, attention_mask, labels):\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    return loss\n",
        "\n",
        "def backward_pass(loss, optimizer):\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "def train_model(model, dataloader, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "            loss = forward_pass(model, input_ids, attention_mask, labels)\n",
        "            backward_pass(loss, optimizer)\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}: Average Loss: {avg_loss}\")\n",
        "\n",
        "train_model(model, dataloader, optimizer, epochs=3)\n",
        "model.save_pretrained('fine_tuned_gpt2_e2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_HIW5wNReYL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "test_texts=[]\n",
        "test_summaries=[]\n",
        "test_df=df1.head(10)\n",
        "\n",
        "for i, row in test_df.iterrows():\n",
        "    test_texts.append(row['Text'])\n",
        "    test_summaries.append(row['Summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzKCKyFYSNWh",
        "outputId": "6cfa66e0-3a62-4b50-cf7b-a28db03395d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s8e46QgzzYw",
        "outputId": "a2d591f7-30d6-41ae-df27-1b9f74f1c6cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE Scores:\n",
            "{'rouge-1': {'r': 0.4133333333333333, 'p': 0.029705520524115948, 'f': 0.05460852674803925}, 'rouge-2': {'r': 0.175, 'p': 0.008955051014456955, 'f': 0.016831678857494748}, 'rouge-l': {'r': 0.3933333333333333, 'p': 0.02692774274633817, 'f': 0.04973047796755146}}\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    return tokenizer\n",
        "\n",
        "def generate_summary(model, tokenizer, sequence, max_length):\n",
        "    inputs = tokenizer.encode(sequence, return_tensors='pt', max_length=max_length, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, early_stopping=True)\n",
        "    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_summary\n",
        "\n",
        "def calculate_rouge(generated_summaries, given_summaries):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(generated_summaries, given_summaries, avg=True)\n",
        "    return scores\n",
        "\n",
        "model_path = \"fine_tuned_gpt2\"\n",
        "model = load_model(model_path)\n",
        "tokenizer = load_tokenizer(model_path)\n",
        "\n",
        "generated_summaries = []\n",
        "test_summaries = []\n",
        "generated_summaries\n",
        "\n",
        "def extract_review_sentiment(summary):\n",
        "    remove=summary.find(\"None\")\n",
        "    remove_word=\"None\"\n",
        "    if remove!=-1:\n",
        "      new_summary=summary[remove + len(remove_word):].strip()\n",
        "    else:\n",
        "      new_summary=\"None\"\n",
        "    return new_summary\n",
        "for text in test_texts:\n",
        "    generated_summary = generate_summary(model, tokenizer, text, max_length=500)\n",
        "    generated_summary = extract_review_sentiment(generated_summary)  # Adjust max_length as needed\n",
        "    generated_summaries.append(generated_summary)\n",
        "\n",
        "test_summaries = test_df['Summary'].tolist()\n",
        "\n",
        "rouge_scores = calculate_rouge(generated_summaries, test_summaries)\n",
        "\n",
        "print(\"ROUGE Scores:\")\n",
        "print(rouge_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxFIeGigTXmN",
        "outputId": "b08bde1e-163e-41e3-bec3-8aa7b480ddf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tried carroll shelbys chili kit earlier week excellent tried one tonight using cut meat disappointed one peppery enough seemed lack cumin garlic firey bland thin tasting rich',\n",
              " 'upming authentic mexican style salsa mpany formerly tucson arizona located bend oregon wanted make gift basket customer needed product fill garden eatin blue tortilla chip fit theme hit mark ntinue product additional gift basket well need business grows southwestsalsam',\n",
              " 'generally love haribo product one didnt like flavor texture son however thought okay',\n",
              " 'youre looking bargain basement cholate isnt want try taste youll never forget probably buy cant go wrong simply best ive ever bar none expensive so acura worth',\n",
              " 'favorite uncle diabetic always feel awful invite birthday cant join eating cake rest u picked package used okie crust cream pie jello sugarfree instant pudding pie filling cholate ounce box pack whipped cream lightly sweetened nonsugar sweetener result gorgeous tasted pretty good even u sugar told know cant sugar told sugar free dug gusto said even birthday feel like got best presentit easy make crust favorite part gave extra package oky bought enjoyed lot toolove making much easier make special treat uncle',\n",
              " 'got week ago say almost think ive wasted money incredibly disappointing drinki wasnt expecting much like expecting taste like freshly made professional drink even cup pumpkin spice ffee wawa expected something would taste good importantly expecting something actually taste pumpkin spice didnt get drink decent enough im willing bring work let workers drink im mean enough bring nasty tasting thing finish definitely something know wont finishing ownmy remmendation absolutely must try drink wait go sale like drink mhi put disappointing',\n",
              " 'price pay amazon prime auto ship mpare oz bottle product store little box cheaper deal',\n",
              " 'bought birthday gift dog plastic pretty thin love tennis ball playing fetch thought would great brain teaser figured lift door get tennis ball go grab end rolling back trunk another hole get irritated slide easily hardwood floor end pushing around room trying get ball put rug keep sliding around door dont open easily im going try put rubber slip grip stuff bottom see help keep place hardwood floor better try dig tennis ball top opening since door dont work well end knocking whole top portion',\n",
              " 'greenies pill pocket way give dog med one pill pocket get two capsule powder supplement large chewable tablet get morning never spit taken use two bag month needed cheaper source local retail store looked online amazon best price buy six month supply doesnt break bank good product great price super quick delivery',\n",
              " 'im glad finally see amazon selling fulfilling product ive buying albertsons lately getting roughly price amazon urse wont pay sale tax get shipped via ups nd day air free since im amazon prime member hope amazon lower price well making one subscribe save product ordered pack lemon cholate spearmint ill happy lower around le per bottle would order stuff like crazy hate drinking regular water seriously trying get diet sodasps youre wondering cholate flavor awesome sound nasty actually good taste bit like drinking water one green andes candy mouth youre trying cut even get sweet good water drink actually satisfies sweet craving']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_summaries"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}